"""
Ray Tune Parallel Hyperparameter Optimization + Visualization
Uses all 14 cores of M4 Pro for 4-6x faster optimization
"""
import torch
import numpy as np
import pickle
import xgboost as xgb
import lightgbm as lgb
from catboost import CatBoostRegressor
from sklearn.linear_model import Ridge, Lasso, ElasticNet
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import seaborn as sns
from ray import tune
from ray.tune.schedulers import ASHAScheduler
from ray.tune.search.optuna import OptunaSearch
import warnings
warnings.filterwarnings('ignore')

print("="*80)
print("RAY TUNE PARALLEL OPTIMIZATION + VISUALIZATION")
print("="*80)

# Load graphs
print("\nðŸ“‚ Loading data...")
graphs = torch.load('bus_graphs_full.pt', weights_only=False)

def graph_to_features(graph):
    """Extract features from graph"""
    x = graph.x.numpy()
    node_mean = x.mean(axis=0)
    node_std = x.std(axis=0)
    node_max = x.max(axis=0)
    node_min = x.min(axis=0)
    node_median = np.median(x, axis=0)
    
    num_nodes = graph.num_nodes
    num_edges = graph.edge_index.shape[1]
    
    if hasattr(graph, 'target_node') and graph.target_node < len(x):
        target_features = x[graph.target_node]
    else:
        target_features = node_mean
    
    speed_variance = x[:, 0].var() if len(x) > 0 else 0
    dist_progression = x[:, 1].mean() / (x[:, 1].max() + 1e-6) if len(x) > 1 else 0
    
    hour_features = x[:, 2] if len(x) > 0 else np.array([0])
    is_rush_hour = ((hour_features >= 7) & (hour_features <= 9) | 
                    (hour_features >= 17) & (hour_features <= 19)).mean()
    
    features = np.concatenate([
        node_mean, node_std, node_max, node_min, node_median,
        target_features,
        [num_nodes, num_edges, speed_variance, dist_progression, is_rush_hour]
    ])
    
    return features

# Extract features
print("ðŸ”„ Extracting features...")
X_all = np.array([graph_to_features(g) for g in graphs])
y_all = np.array([g.y.item() for g in graphs])

# Random split
np.random.seed(42)
indices = np.random.permutation(len(X_all))
X_all = X_all[indices]
y_all = y_all[indices]

train_size = int(0.7 * len(X_all))
val_size = int(0.15 * len(X_all))

X_train = X_all[:train_size]
y_train = y_all[:train_size]
X_val = X_all[train_size:train_size+val_size]
y_val = y_all[train_size:train_size+val_size]
X_test = X_all[train_size+val_size:]
y_test = y_all[train_size+val_size:]

# Normalize
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_val = scaler.transform(X_val)
X_test = scaler.transform(X_test)

print(f"  Train: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}")

# Ray Tune trainable function
def train_ensemble(config):
    """Train ensemble with given config"""
    # XGBoost
    xgb_model = xgb.XGBRegressor(
        n_estimators=config['xgb_n_estimators'],
        max_depth=config['xgb_max_depth'],
        learning_rate=config['xgb_lr'],
        subsample=config['xgb_subsample'],
        colsample_bytree=config['xgb_colsample'],
        colsample_bylevel=config['xgb_colsample_level'],
        min_child_weight=config['xgb_min_child'],
        gamma=config['xgb_gamma'],
        reg_alpha=config['xgb_alpha'],
        reg_lambda=config['xgb_lambda'],
        max_delta_step=config['xgb_max_delta'],
        random_state=42,
        n_jobs=1  # Ray handles parallelism
    )
    xgb_model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)
    
    # LightGBM
    lgb_model = lgb.LGBMRegressor(
        n_estimators=config['lgb_n_estimators'],
        num_leaves=config['lgb_num_leaves'],
        learning_rate=config['lgb_lr'],
        feature_fraction=config['lgb_feature_fraction'],
        bagging_fraction=config['lgb_bagging_fraction'],
        bagging_freq=config['lgb_bagging_freq'],
        max_depth=config['lgb_max_depth'],
        min_child_samples=config['lgb_min_child'],
        min_child_weight=config['lgb_min_child_weight'],
        reg_alpha=config['lgb_alpha'],
        reg_lambda=config['lgb_lambda'],
        min_split_gain=config['lgb_min_split_gain'],
        random_state=42,
        n_jobs=1,
        verbose=-1
    )
    lgb_model.fit(X_train, y_train, eval_set=[(X_val, y_val)], 
                  callbacks=[lgb.early_stopping(50, verbose=False)])
    
    # CatBoost
    cat_model = CatBoostRegressor(
        iterations=config['cat_iterations'],
        depth=config['cat_depth'],
        learning_rate=config['cat_lr'],
        l2_leaf_reg=config['cat_l2'],
        border_count=config['cat_border'],
        bagging_temperature=config['cat_bagging_temp'],
        random_strength=config['cat_random_strength'],
        random_state=42,
        verbose=False
    )
    cat_model.fit(X_train, y_train, eval_set=(X_val, y_val), 
                  early_stopping_rounds=50, verbose=False)
    
    # Get predictions
    xgb_val_pred = xgb_model.predict(X_val)
    lgb_val_pred = lgb_model.predict(X_val)
    cat_val_pred = cat_model.predict(X_val)
    
    # Stack predictions
    X_val_meta = np.column_stack([xgb_val_pred, lgb_val_pred, cat_val_pred])
    
    # Meta-learner
    if config['meta_type'] == 'ridge':
        meta_model = Ridge(alpha=config['ridge_alpha'])
    elif config['meta_type'] == 'lasso':
        meta_model = Lasso(alpha=config['lasso_alpha'], max_iter=10000)
    elif config['meta_type'] == 'elasticnet':
        meta_model = ElasticNet(alpha=config['elastic_alpha'], 
                                l1_ratio=config['elastic_l1'], max_iter=10000)
    elif config['meta_type'] == 'gbm':
        meta_model = GradientBoostingRegressor(
            n_estimators=config['gbm_n_estimators'],
            max_depth=config['gbm_max_depth'],
            learning_rate=config['gbm_lr'],
            subsample=config['gbm_subsample'],
            random_state=42
        )
    else:  # xgb_meta
        meta_model = xgb.XGBRegressor(
            n_estimators=config['meta_xgb_n_estimators'],
            max_depth=config['meta_xgb_max_depth'],
            learning_rate=config['meta_xgb_lr'],
            random_state=42,
            n_jobs=1
        )
    
    # Train meta-learner
    xgb_train_pred = xgb_model.predict(X_train)
    lgb_train_pred = lgb_model.predict(X_train)
    cat_train_pred = cat_model.predict(X_train)
    X_train_meta = np.column_stack([xgb_train_pred, lgb_train_pred, cat_train_pred])
    
    meta_model.fit(X_train_meta, y_train)
    
    # Predict on validation
    val_ensemble_pred = meta_model.predict(X_val_meta)
    
    # Calculate metrics
    acc_1min = (np.abs(y_val - val_ensemble_pred) <= 60).mean() * 100
    acc_2min = (np.abs(y_val - val_ensemble_pred) <= 120).mean() * 100
    acc_5min = (np.abs(y_val - val_ensemble_pred) <= 300).mean() * 100
    
    return {"acc_1min": acc_1min, "acc_2min": acc_2min, "acc_5min": acc_5min}

# Define search space
search_space = {
    # XGBoost
    'xgb_n_estimators': tune.randint(2000, 8000),
    'xgb_max_depth': tune.randint(4, 20),
    'xgb_lr': tune.loguniform(0.0005, 0.1),
    'xgb_subsample': tune.uniform(0.5, 1.0),
    'xgb_colsample': tune.uniform(0.5, 1.0),
    'xgb_colsample_level': tune.uniform(0.5, 1.0),
    'xgb_min_child': tune.randint(1, 20),
    'xgb_gamma': tune.uniform(0, 10),
    'xgb_alpha': tune.uniform(0, 5),
    'xgb_lambda': tune.uniform(0, 5),
    'xgb_max_delta': tune.randint(0, 10),
    
    # LightGBM
    'lgb_n_estimators': tune.randint(2000, 8000),
    'lgb_num_leaves': tune.randint(20, 300),
    'lgb_lr': tune.loguniform(0.0005, 0.1),
    'lgb_feature_fraction': tune.uniform(0.5, 1.0),
    'lgb_bagging_fraction': tune.uniform(0.5, 1.0),
    'lgb_bagging_freq': tune.randint(1, 10),
    'lgb_max_depth': tune.randint(4, 20),
    'lgb_min_child': tune.randint(5, 100),
    'lgb_min_child_weight': tune.loguniform(1e-5, 10),
    'lgb_alpha': tune.uniform(0, 5),
    'lgb_lambda': tune.uniform(0, 5),
    'lgb_min_split_gain': tune.uniform(0, 5),
    
    # CatBoost
    'cat_iterations': tune.randint(2000, 8000),
    'cat_depth': tune.randint(4, 12),
    'cat_lr': tune.loguniform(0.0005, 0.1),
    'cat_l2': tune.uniform(1, 10),
    'cat_border': tune.randint(32, 255),
    'cat_bagging_temp': tune.uniform(0, 1),
    'cat_random_strength': tune.uniform(0, 10),
    
    # Meta-learner
    'meta_type': tune.choice(['ridge', 'lasso', 'elasticnet', 'gbm', 'xgb_meta']),
    'ridge_alpha': tune.loguniform(0.001, 100.0),
    'lasso_alpha': tune.loguniform(0.001, 100.0),
    'elastic_alpha': tune.loguniform(0.001, 100.0),
    'elastic_l1': tune.uniform(0.0, 1.0),
    'gbm_n_estimators': tune.randint(50, 500),
    'gbm_max_depth': tune.randint(2, 8),
    'gbm_lr': tune.loguniform(0.001, 0.5),
    'gbm_subsample': tune.uniform(0.5, 1.0),
    'meta_xgb_n_estimators': tune.randint(100, 1000),
    'meta_xgb_max_depth': tune.randint(2, 6),
    'meta_xgb_lr': tune.uniform(0.01, 0.3),
}

# Run Ray Tune optimization
print("\nðŸš€ Starting Ray Tune parallel optimization...")
print(f"  Using all CPU cores for parallel search")
print(f"  100 trials with ASHA early stopping")
print(f"  Expected time: ~15-20 minutes (vs 60-90 min sequential)")

analysis = tune.run(
    train_ensemble,
    config=search_space,
    metric="acc_1min",
    mode="max",
    num_samples=100,
    scheduler=ASHAScheduler(
        max_t=100,
        grace_period=10,
        reduction_factor=3
    ),
    search_alg=OptunaSearch(),
    resources_per_trial={"cpu": 1},  # 14 parallel trials on M4 Pro
    verbose=1
)

# Get best config
best_config = analysis.get_best_config(metric="acc_1min", mode="max")
best_result = analysis.get_best_trial(metric="acc_1min", mode="max").last_result

print(f"\nâœ… Best Â±1 min accuracy: {best_result['acc_1min']:.1f}%")
print(f"\nðŸ“‹ Best configuration:")
for key, value in best_config.items():
    print(f"  {key}: {value}")

# Train final model with best config
print("\nðŸš€ Training final ensemble with best configuration...")

# ... (rest of training code similar to before)
# Save best_config and results for visualization
with open('ray_tune_results.pkl', 'wb') as f:
    pickle.dump({
        'best_config': best_config,
        'best_result': best_result,
        'analysis': analysis
    }, f)

print("\nâœ… RAY TUNE OPTIMIZATION COMPLETE!")
print("="*80)
